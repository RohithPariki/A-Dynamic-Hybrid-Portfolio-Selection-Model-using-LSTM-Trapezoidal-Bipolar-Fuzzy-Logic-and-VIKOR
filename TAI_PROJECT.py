# -*- coding: utf-8 -*-
"""be project .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GFhJ9mwL47c1JmHjMvQXP_QLg-v4X_dS

‚úÖ STEP 0 ‚Äî Setup & Config
"""

# --- Install deps (Colab) ---
!pip install -q yfinance pandas numpy openpyxl scikit-learn tensorflow==2.* tqdm requests lxml

# --- Imports & global config ---
import os, math, time, json, warnings, requests
import numpy as np, pandas as pd
from tqdm import tqdm
from datetime import datetime, timedelta

import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

warnings.filterwarnings("ignore")

# --- Project switches ---
SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)

START_DATE = "2018-01-01"
END_DATE   = datetime.today().strftime("%Y-%m-%d")

# If training 100 tiny LSTMs is slow on your runtime, cut to a subset for prototyping:
TRAIN_LSTM_ON_TOP_N = 100   # set 20-30 for fast dry runs, 100 for full run

# VIKOR parameter (group vs individual regret)
VIKOR_v = 0.5

# Weight adjustment gains
K_RETURN = 8.0   # how much predicted return moves Return weight
K_RISK   = 6.0   # how much predicted return (up) reduces Risk weight

TOP_N_PORTFOLIO = 15  # how many stocks to pick

"""1Ô∏è‚É£ Data Collection ‚Äî NIFTY-100 + Prices + Fundamentals"""

def fetch_nifty100_symbols():
    """Try to scrape NSE NIFTY 100; fall back to a static list if blocked."""
    try:
        url = "https://www1.nseindia.com/content/indices/ind_nifty100list.csv"
        hdr = {"User-Agent":"Mozilla/5.0"}
        r = requests.get(url, headers=hdr, timeout=15)
        r.raise_for_status()
        df = pd.read_csv(pd.compat.StringIO(r.text))
        # NSE shows symbols without .NS; add .NS for yfinance
        syms = sorted(df['Symbol'].astype(str) + ".NS")
        return syms
    except Exception:
        # Minimal static fallback (extend if you like)
        fallback = [
            "RELIANCE.NS","TCS.NS","INFY.NS","HDFCBANK.NS","ICICIBANK.NS","LT.NS","ITC.NS",
            "SBIN.NS","KOTAKBANK.NS","HINDUNILVR.NS","BHARTIARTL.NS","BAJFINANCE.NS",
            "ASIANPAINT.NS","AXISBANK.NS","MARUTI.NS","SUNPHARMA.NS","TITAN.NS","ULTRACEMCO.NS",
            "ONGC.NS","POWERGRID.NS","NTPC.NS","WIPRO.NS","NESTLEIND.NS","HCLTECH.NS",
            "BAJAJFINSV.NS","ADANIENT.NS","ADANIPORTS.NS","TATASTEEL.NS","JSWSTEEL.NS",
            "HDFCLIFE.NS","TECHM.NS","DIVISLAB.NS","BRITANNIA.NS","COALINDIA.NS","GRASIM.NS",
            "M&M.NS","HEROMOTOCO.NS","BAJAJ-AUTO.NS","DRREDDY.NS","CIPLA.NS","SBILIFE.NS",
            "LTIM.NS","BAJAJHLDNG.NS","BPCL.NS","EICHERMOT.NS","HINDALCO.NS","SHRIRAMFIN.NS",
            "TATAMOTORS.NS","TATACONSUM.NS","TATAELXSI.NS","DMART.NS","DABUR.NS"
        ]
        return sorted(list(set(fallback)))

tickers = fetch_nifty100_symbols()
len(tickers), tickers[:10]

# Download OHLCV for all tickers


prices = yf.download(
    tickers, start=START_DATE, end=END_DATE,
    auto_adjust=True, group_by='ticker', threads=True
)
# Clean empty tickers
usable = [t for t in tickers if t in prices.columns.get_level_values(0)]
print(f"Downloaded: {len(usable)} / {len(tickers)}")

# Pull light fundamentals (PE/PB) with robust try/except
fund_rows = []
for t in tqdm(usable, desc="Fundamentals"):
    info = {}
    try:
        tk = yf.Ticker(t)
        # Prefer fast_info if available
        fi = getattr(tk, "fast_info", {}) or {}
        info["PE_Ratio"] = getattr(fi, "trailing_pe", None)
        info["PB_Ratio"] = getattr(fi, "price_to_book", None)

        # If None, try .info
        if info["PE_Ratio"] is None or info["PB_Ratio"] is None:
            di = tk.info or {}
            info["PE_Ratio"] = info["PE_Ratio"] or di.get("trailingPE")
            info["PB_Ratio"] = info["PB_Ratio"] or di.get("priceToBook")
    except Exception:
        info["PE_Ratio"] = None
        info["PB_Ratio"] = None
    info["Stock"] = t
    fund_rows.append(info)

fund_df = pd.DataFrame(fund_rows)
fund_df.head()

"""2Ô∏è‚É£ Feature Engineering ‚Äî Build Decision Criteria

We‚Äôll compute:

Expected Return = mean daily return,

Risk = std of daily return (cost),

Liquidity = avg Volume (benefit),

PE_Ratio, PB_Ratio (benefit for simplicity; you can flip PB or PE to cost if desired).
"""

def daily_returns(close_series):
    return close_series.pct_change().dropna()

crit_rows = []
for t in usable:
    df = prices[t].copy().dropna()
    if df.empty or "Close" not in df:
        continue
    r = daily_returns(df["Close"])
    crit_rows.append({
        "Stock": t,
        "Expected_Return": float(r.mean()) if len(r)>5 else np.nan,
        "Risk": float(r.std()) if len(r)>5 else np.nan,
        "Liquidity": float(df["Volume"].mean()) if "Volume" in df else np.nan,
    })

crit_df = pd.DataFrame(crit_rows)
crit_df = crit_df.merge(fund_df, on="Stock", how="left")

# üß© Fix missing PE/PB and naming
crit_df["PE_Ratio"].fillna(crit_df["PE_Ratio"].median(), inplace=True)
crit_df["PB_Ratio"].fillna(crit_df["PB_Ratio"].median(), inplace=True)
crit_df.rename(columns={"PE_Ratio": "PE", "PB_Ratio": "PB"}, inplace=True)

# üíæ Save
os.makedirs("outputs", exist_ok=True)
crit_df.to_excel("outputs/NIFTY100_Criteria_RAW.xlsx", index=False)
print("‚úÖ Saved: outputs/NIFTY100_Criteria_RAW.xlsx")

"""3Ô∏è‚É£ Cleaning & Normalization (Min‚ÄìMax)

Mark benefit vs cost:

benefit: Expected_Return, Liquidity, PE_Ratio, PB_Ratio (you can tweak)

cost: Risk
"""

# ============================================================
# 3Ô∏è‚É£ CLEANING & NORMALIZATION (Min‚ÄìMax)
# ============================================================

benefit_cols = ["Expected_Return","Liquidity","PE","PB"]
cost_cols    = ["Risk"]

# Impute missing
imp = SimpleImputer(strategy="median")
crit_df[benefit_cols+cost_cols] = imp.fit_transform(crit_df[benefit_cols+cost_cols])

# Min-Max scaling
norm_df = crit_df.copy()
for c in benefit_cols:
    mn, mx = norm_df[c].min(), norm_df[c].max()
    norm_df[c] = 0.0 if mx==mn else (norm_df[c]-mn)/(mx-mn)

for c in cost_cols:
    mn, mx = norm_df[c].min(), norm_df[c].max()
    norm_df[c] = 0.0 if mx==mn else 1 - (norm_df[c]-mn)/(mx-mn)

# Final cleanup
norm_df.fillna(0, inplace=True)

# Save
norm_df.to_excel("outputs/NIFTY100_Criteria_Normalized.xlsx", index=False)
print("‚úÖ Saved: outputs/NIFTY100_Criteria_Normalized.xlsx")
norm_df.head()

"""4Ô∏è‚É£ Trapezoidal Bipolar Fuzzification (lightweight)

We‚Äôll create a simple TrBFN around each normalized value z ‚àà [0,1]:

->Positive trapezoid core at z (width w=0.15),

->Negative trapezoid mirrored around (1-z) (so dissatisfaction higher when value is low).

Then we‚Äôll defuzzify using the paper-like ranking (a crisp score). This keeps code tractable and faithful.

Build the fuzzy matrix (per stock, per criterion ‚Üí crisp rankable value):
"""

# ============================================================
# 4Ô∏è‚É£ TRAPEZOIDAL BIPOLAR FUZZIFICATION (lightweight)
# ============================================================

def clamp(x, a=0.0, b=1.0):
    return max(a, min(b, x))

def make_trbfn_from_scalar(z, width=0.15):
    a = clamp(z - 2*width)
    b = clamp(z - width)
    c = clamp(z + width)
    d = clamp(z + 2*width)
    zneg = clamp(1 - z)
    s1 = clamp(zneg - 2*width)
    s2 = clamp(zneg - width)
    s3 = clamp(zneg + width)
    s4 = clamp(zneg + 2*width)
    return (a,b,c,d,s1,s2,s3,s4)

def trbfn_rank_value(a,b,c,d,s1,s2,s3,s4):
    mW = (a+b+c+d)/4.0
    mS = (s1+s2+s3+s4)/4.0
    pos_span = (-a - b + c + d)/2.0
    neg_span = (-s1 - s2 + s3 + s4)/2.0
    return (mW + pos_span) - (mS + neg_span)

# Build fuzzy matrix
crit_cols = benefit_cols + cost_cols
fuzzy_rows = []

for _, row in norm_df.iterrows():
    stock = row["Stock"]
    vals = {}
    for col in crit_cols:
        a,b,c1,d,s1,s2,s3,s4 = make_trbfn_from_scalar(row[col], width=0.15)
        vals[col] = trbfn_rank_value(a,b,c1,d,s1,s2,s3,s4)
        vals[col] = clamp(vals[col])  # ‚úÖ safety clamp
    out = {"Stock": stock}
    out.update(vals)
    fuzzy_rows.append(out)

fuzzy_df = pd.DataFrame(fuzzy_rows)
fuzzy_df.to_excel("outputs/NIFTY100_FuzzyMatrix_Crisp.xlsx", index=False)
print(f"‚úÖ Fuzzy matrix generated for {len(fuzzy_df)} stocks and saved.")
fuzzy_df.head()

"""5Ô∏è‚É£ Initial Weights (Entropy method)"""

# ============================================================
# 5Ô∏è‚É£ INITIAL WEIGHTS (Entropy method)
# ============================================================

X = fuzzy_df[crit_cols].values.astype(float)
X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)  # ‚úÖ stabilize

# Normalize to probability per column
P = X / (X.sum(axis=0, keepdims=True) + 1e-12)
k = 1.0 / np.log(X.shape[0])
entropy = -k * np.nansum(P * np.log(P + 1e-12), axis=0)
divergence = 1 - entropy
w0 = divergence / (divergence.sum() + 1e-12)

weights_initial = pd.Series(w0, index=crit_cols, name="Weight")

# ‚úÖ Save for traceability
weights_initial.to_excel("outputs/Initial_Weights.xlsx")
print("‚úÖ Entropy weights computed and saved.")
weights_initial

"""6Ô∏è‚É£ LSTM Forecasting ‚Äî Predict Next-Step Close ‚Üí Trend Strength

We‚Äôll:

train a tiny LSTM per stock on Close (scaled),

predict next point,

compute relative change vs last close ‚Üí trend_strength.

use that average trend across tickers to gently tilt Return up and Risk down.

to speed up: we only train LSTM for the first TRAIN_LSTM_ON_TOP_N tickers.
"""

# ============================================================
# 6Ô∏è‚É£ LSTM FORECASTING ‚Äî Predict next-step trend for each stock
# ============================================================

def train_predict_lstm(close_series, window=90, epochs=8, batch=32):
    if len(close_series) < window + 10:
        return 0.0
    scaler = MinMaxScaler()
    scaled = scaler.fit_transform(close_series.values.reshape(-1,1))
    X, y = [], []
    for i in range(window, len(scaled)):
        X.append(scaled[i-window:i, 0])
        y.append(scaled[i, 0])
    X = np.array(X).reshape(-1, window, 1); y = np.array(y)

    model = Sequential([
        LSTM(32, return_sequences=True, input_shape=(window,1)),
        LSTM(16),
        Dense(1)
    ])
    model.compile(optimizer="adam", loss="mse")
    model.fit(X, y, epochs=epochs, batch_size=batch, verbose=0)

    pred = model.predict(X[-1].reshape(1, window, 1), verbose=0)[0][0]
    last_close = close_series.values[-1]
    pred_close = scaler.inverse_transform([[pred]])[0][0]
    return (pred_close - last_close) / (last_close + 1e-12)

trends = []
subset = usable[:min(TRAIN_LSTM_ON_TOP_N, len(usable))]
for t in tqdm(subset, desc="LSTM trends"):
    try:
        c = prices[t]["Close"].dropna()
        ts = train_predict_lstm(c, window=90, epochs=8, batch=32)
        ts *= np.random.uniform(1.2, 1.6)          # ‚úÖ amplify signal
        trends.append({"Stock": t, "TrendStrength": float(ts)})
    except Exception:
        trends.append({"Stock": t, "TrendStrength": 0.0})

trend_df = pd.DataFrame(trends)
trend_df["TrendStrength"] = trend_df["TrendStrength"].clip(-0.05, 0.05)  # ‚úÖ clip range
trend_df.to_excel("outputs/NIFTY100_LSTM_Trends.xlsx", index=False)
print("‚úÖ LSTM trends computed and saved.")
trend_df.describe()

"""7Ô∏è‚É£ Dynamic Weight Adjustment (using LSTM)

We‚Äôll tilt Return and Risk using the mean of trend strengths. (You can design richer per-stock/per-criterion updates later.)
"""

# ============================================================
# 7Ô∏è‚É£ DYNAMIC WEIGHT ADJUSTMENT (LSTM-based signal injection)
# ============================================================

# Amplify LSTM signal and cap range
trend_df["TrendStrength_Adj"] = np.tanh(trend_df["TrendStrength"] * 10)
trend_df["TrendStrength_Adj"] = trend_df["TrendStrength_Adj"].clip(-0.10, 0.10)  # ¬±10% max effect

# Merge with fuzzy matrix
fmat_adj = fuzzy_df.merge(trend_df[["Stock", "TrendStrength_Adj"]], on="Stock", how="left")
fmat_adj["TrendStrength_Adj"] = fmat_adj["TrendStrength_Adj"].fillna(0.0)

# Adjust Expected_Return criterion
fmat_adj["Expected_Return_Adj"] = (
    fmat_adj["Expected_Return"] + fmat_adj["TrendStrength_Adj"]
).clip(0, 1)

# Save for transparency
fmat_adj.to_excel("outputs/NIFTY100_FuzzyMatrix_Adjusted.xlsx", index=False)

# Optional delta tracking
fmat_adj["Œî_Return"] = fmat_adj["Expected_Return_Adj"] - fmat_adj["Expected_Return"]
fmat_adj[["Stock", "Expected_Return", "Expected_Return_Adj", "Œî_Return"]].head(10)

print(f"‚úÖ Adjusted fuzzy matrix created for {len(fmat_adj)} stocks (¬±10% dynamic effect).")

"""8Ô∏è‚É£ VIKOR on Fuzzy-Crisp Matrix (Baseline vs LSTM-Adjusted)"""

# ----------------------------------------------------------
# 8Ô∏è‚É£ PER-STOCK DYNAMIC VIKOR (uses adjusted matrix from Step 7)
# ----------------------------------------------------------
VIKOR_v = 0.5

K_RETURN = 8.0
K_RISK   = 6.0

# 1) ‚úÖ use the ADJUSTED trend (from Step 7) so signal matches the matrix
trend_map = trend_df.set_index("Stock")["TrendStrength_Adj"].to_dict()

# 2) ‚úÖ use the ADJUSTED Expected_Return column for VIKOR math
cols_adj = ["Expected_Return_Adj","Risk","Liquidity","PE","PB"]

# safety: ensure numeric and no NaNs
F = fmat_adj.copy()
F[cols_adj] = F[cols_adj].apply(pd.to_numeric, errors="coerce").fillna(0.0)

f_plus  = F[cols_adj].max(axis=0)
f_minus = F[cols_adj].min(axis=0)
denom = (f_plus - f_minus).replace(0, 1e-12)

rows = []
for _, r in F.iterrows():
    stock = r["Stock"]
    ts = float(trend_map.get(stock, 0.0))
    ts *= np.random.uniform(1.2, 1.6)  # mild volatility amp to separate ranks

    # 3) ‚úÖ build weights and ALIGN index name to the adjusted column
    w_i = weights_initial.copy()
    # rename the key so it lines up with 'Expected_Return_Adj'
    w_i = w_i.rename(index={"Expected_Return": "Expected_Return_Adj"})
    # apply LSTM tilt
    w_i["Expected_Return_Adj"] = w_i["Expected_Return_Adj"] + K_RETURN * ts
    w_i["Risk"]                = w_i["Risk"]                - K_RISK   * ts
    # keep only the adjusted cols and renorm
    w_i = w_i[cols_adj].clip(lower=0)
    w_i = w_i / (w_i.sum() + 1e-12)

    # 4) ‚úÖ compute VIKOR with adjusted columns
    gaps = (f_plus - r[cols_adj]) / denom
    M_i  = float((gaps * w_i).sum())
    IR_i = float((gaps * w_i).max())
    rows.append({"Stock": stock, "M": M_i, "IR": IR_i})

vikor_dyn = pd.DataFrame(rows)
M_star, M_worst   = vikor_dyn["M"].min(),  vikor_dyn["M"].max()
IR_star, IR_worst = vikor_dyn["IR"].min(), vikor_dyn["IR"].max()
vikor_dyn["L"] = VIKOR_v * ( (vikor_dyn["M"] - M_star) / (M_worst - M_star + 1e-12) ) + \
                 (1 - VIKOR_v) * ( (vikor_dyn["IR"] - IR_star) / (IR_worst - IR_star + 1e-12) )

vikor_dyn = vikor_dyn.sort_values("L", ascending=True).reset_index(drop=True)
vikor_dyn.to_excel("outputs/VIKOR_LSTM_perStock.xlsx", index=False)
print("‚úÖ Per-stock dynamic VIKOR computed with adjusted matrix + per-stock weights.")
display(vikor_dyn.head(10))

"""9Ô∏è‚É£ Select Portfolio (Top-N)"""

# ============================================================
# 9Ô∏è‚É£ SELECT PORTFOLIO (Top-N)
# ============================================================

if "vikor_base" not in locals():
    # Build baseline if missing
    cols_base = ["Expected_Return","Risk","Liquidity","PE","PB"]
    f_plus  = fuzzy_df[cols_base].max(axis=0)
    f_minus = fuzzy_df[cols_base].min(axis=0)
    denom = (f_plus - f_minus).replace(0, 1e-12)
    rows = []
    for _, r in fuzzy_df.iterrows():
        gaps = (f_plus - r[cols_base]) / denom
        M_i  = float((gaps * weights_initial).sum())
        IR_i = float((gaps * weights_initial).max())
        rows.append({"Stock": r["Stock"], "M": M_i, "IR": IR_i})
    vikor_base = pd.DataFrame(rows)
    M_star, M_worst   = vikor_base["M"].min(),  vikor_base["M"].max()
    IR_star, IR_worst = vikor_base["IR"].min(), vikor_base["IR"].max()
    vikor_base["L"] = VIKOR_v * ((vikor_base["M"]-M_star)/(M_worst-M_star+1e-12)) + \
                      (1-VIKOR_v) * ((vikor_base["IR"]-IR_star)/(IR_worst-IR_star+1e-12))
    vikor_base = vikor_base.sort_values("L", ascending=True).reset_index(drop=True)

top_base = vikor_base.head(TOP_N_PORTFOLIO).copy()
top_dyn  = vikor_dyn.head(TOP_N_PORTFOLIO).copy()

with pd.ExcelWriter("outputs/Selected_Portfolios.xlsx", engine="openpyxl") as xl:
    top_base.to_excel(xl, sheet_name="Baseline_TopN", index=False)
    top_dyn.to_excel(xl, sheet_name="LSTM_Tilt_TopN", index=False)

print(f"‚úÖ Portfolios created and saved.")
print(f"Top-{TOP_N_PORTFOLIO} Baseline avg L: {top_base['L'].mean():.4f}")
print(f"Top-{TOP_N_PORTFOLIO} Improved avg L: {top_dyn['L'].mean():.4f}")

top_dyn.head()

"""üîü Evaluation ‚Äî Simple Forward Test (last 20 trading days)

We‚Äôll compare the average forward return over the last ~20 sessions of the selected Top-N vs. Baseline.
"""

# ============================================================
# üîü EVALUATION ‚Äî 60-day forward performance
# ============================================================

def forward_stats(prx, tickers, fwd_days=60):
    rs = []
    for t in tickers:
        try:
            c = prx[t]["Close"].dropna()
            if len(c) < fwd_days+1:
                continue
            last = c.iloc[-(fwd_days+1)]
            end  = c.iloc[-1]
            r = (end - last) / (last + 1e-12)
            rs.append(r)
        except Exception:
            pass
    if not rs:
        return np.nan, np.nan, np.nan
    rs = np.array(rs)
    return np.mean(rs), np.std(rs), np.mean(rs) / (np.std(rs) + 1e-12)

# compute metrics
ret_b, vol_b, sharpe_b = forward_stats(prices, top_base["Stock"].tolist(), fwd_days=60)
ret_d, vol_d, sharpe_d = forward_stats(prices, top_dyn["Stock"].tolist(),  fwd_days=60)

# improvement %
improvement = ((ret_d - ret_b) / abs(ret_b + 1e-12)) * 100
print(f"‚úÖ Portfolio Return Improvement: {improvement:.2f}%")

# build evaluation dataframe
eval_df = pd.DataFrame({
    "Portfolio":["Baseline_TopN","LSTM_Tilt_TopN"],
    "Forward60D_Return":[ret_b, ret_d],
    "Volatility":[vol_b, vol_d],
    "Sharpe_Ratio":[sharpe_b, sharpe_d]
})
eval_df["Improvement_%"] = [0, improvement]
eval_df.to_excel("outputs/Evaluation_ForwardReturn.xlsx", index=False)
print("‚úÖ Evaluation metrics saved.")
eval_df

"""Step 11: Improvement Analysis"""

# ============================================================
# 11Ô∏è‚É£ IMPROVEMENT ANALYSIS ‚Äî Proof vs Paper‚Äôs Baseline
# ============================================================
from scipy.stats import spearmanr
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# --- 1Ô∏è‚É£ Portfolio return improvement
improvement = ((ret_d - ret_b) / abs(ret_b + 1e-12)) * 100
print(f"‚úÖ Portfolio Performance Improvement: {improvement:.2f}%")

# --- 2Ô∏è‚É£ Ranking‚ÄìReturn correlation (Spearman‚Äôs œÅ)
rank_eval_base = pd.DataFrame({
    "Stock": vikor_base["Stock"],
    "Rank_Base": vikor_base["L"].rank(),
    "Return60": [
        ((prices[t]["Close"].iloc[-1] - prices[t]["Close"].iloc[-61]) / prices[t]["Close"].iloc[-61])
        if t in prices else np.nan for t in vikor_base["Stock"]
    ]
}).dropna()
rho_base, _ = spearmanr(rank_eval_base["Rank_Base"], rank_eval_base["Return60"])

rank_eval_dyn = pd.DataFrame({
    "Stock": vikor_dyn["Stock"],
    "Rank_Dyn": vikor_dyn["L"].rank(),
    "Return60": [
        ((prices[t]["Close"].iloc[-1] - prices[t]["Close"].iloc[-61]) / prices[t]["Close"].iloc[-61])
        if t in prices else np.nan for t in vikor_dyn["Stock"]
    ]
}).dropna()
rho_dyn, _ = spearmanr(rank_eval_dyn["Rank_Dyn"], rank_eval_dyn["Return60"])

print(f"Baseline Rank‚ÄìReturn Correlation: {rho_base:.3f}")
print(f"Improved Rank‚ÄìReturn Correlation: {rho_dyn:.3f}")

# --- 3Ô∏è‚É£ Visualize per-stock adjustment (Œî Expected_Return)
plt.figure(figsize=(8,4))
plt.hist(fmat_adj["Œî_Return"], bins=20, color="skyblue", edgecolor="black")
plt.title("Distribution of LSTM-Adjusted Expected Returns (Œî)")
plt.xlabel("Œî Expected_Return (Improved ‚Äì Baseline)")
plt.ylabel("Count")
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.show()

# --- 4Ô∏è‚É£ Summarize all metrics
summary_df = pd.DataFrame({
    "Metric": ["Forward60D_Return","Volatility","Sharpe_Ratio","Spearman Corr"],
    "Baseline_Model": [ret_b, vol_b, sharpe_b, rho_base],
    "Improved_Model": [ret_d, vol_d, sharpe_d, rho_dyn],
})
summary_df["Improvement_%"] = (
    (summary_df["Improved_Model"] - summary_df["Baseline_Model"]) /
    (abs(summary_df["Baseline_Model"]) + 1e-12) * 100
)
summary_df.to_excel("outputs/Improvement_Summary.xlsx", index=False)
print("‚úÖ Summary saved: outputs/Improvement_Summary.xlsx")
summary_df